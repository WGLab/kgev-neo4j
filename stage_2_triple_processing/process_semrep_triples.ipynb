{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables below should be updated accordingly.  \n",
    "`INPUT_FILE_PATH` is where the .csv file with SemRep relationships is located (i.e. the output from Stage 1).  \n",
    "`OUTPUT_FILE_PATH` is where the processed triples/the output from Stage 2 will be stored (as a .csv file).  \n",
    "`NODE_MAPPING_FILE_PATH` is where node information for the processed triples will be stored (as a .csv file).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = '../data/semrep_relationships.csv'\n",
    "OUTPUT_FILE_PATH = '../data/semrep_relationships_processed.csv'\n",
    "NODE_MAPPING_FILE_PATH = '../data/node_id_mapping.csv'\n",
    "NEO4J_OUTPUT_DIR = '../neo4j-import'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_col = 'subj_preferred_name'\n",
    "rel_col = 'relation'\n",
    "obj_col = 'obj_preferred_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pengja/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# use metadata to sort triples by date and drop duplicate triples from same sentence, keeping one from most recent paper\n",
    "metadata_path = '../data/metadata.csv' # CORD-19 metadata from kaggle\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "metadata_df = metadata_df.rename(columns={'cord_uid': 'paper_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relationships directly outputted by SemRep = 1132905\n"
     ]
    }
   ],
   "source": [
    "kg_df = pd.read_csv(INPUT_FILE_PATH, keep_default_na=False)\n",
    "kg_df = kg_df.merge(metadata_df[['paper_id', 'publish_time']], on='paper_id', how='left').sort_values('publish_time', ascending=False)\n",
    "kg_df = kg_df[[\n",
    "    'paper_id', 'subj_CUI', 'subj_preferred_name', 'subj_semantic_type', 'subj_gene_id', 'subj_gene_name', \\\n",
    "    'subj_original_text', 'subj_negated', 'subj_confidence_score', 'subj_start', 'subj_end' , \\\n",
    "    'relation_type', 'relation', 'relation_negated', 'relation_start', 'relation_end', 'relation_original_text', \\\n",
    "    'obj_CUI', 'obj_preferred_name', 'obj_semantic_type', 'obj_gene_id', 'obj_gene_name', 'obj_original_text', \\\n",
    "    'obj_negated', 'obj_confidence_score', 'obj_start', 'obj_end', \\\n",
    "    'sentence']]\n",
    "print(\"Number of relationships directly outputted by SemRep =\", len(kg_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relationships (after removing empty strings) = 1123654\n"
     ]
    }
   ],
   "source": [
    "# replace whitespace with NaN\n",
    "kg_df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "# remove NaN for subj, relationship, and obj cols\n",
    "kg_df = kg_df.dropna(axis=0, subset=[subj_col, \"subj_original_text\", rel_col, \"relation_original_text\", obj_col, \"obj_original_text\"])\n",
    "print(\"Number of relationships (after removing empty strings) =\", len(kg_df))\n",
    "\n",
    "# check that all subj, verb, obj cells are non-null\n",
    "missing = kg_df[(kg_df[subj_col].isnull()) | (kg_df[rel_col].isnull()) | (kg_df[obj_col].isnull())]\n",
    "assert len(missing) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relationships (after dropping duplicates) = 918936\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates\n",
    "kg_df['sentence_lower'] = kg_df['sentence'].str.lower().str.strip()\n",
    "kg_df = kg_df.drop_duplicates(subset=['subj_start', 'subj_end', 'obj_start', 'obj_end', 'relation_start', 'relation_end', 'sentence_lower', subj_col, rel_col, obj_col])\n",
    "\n",
    "print(\"Number of relationships (after dropping duplicates) =\", len(kg_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase original text\n",
    "kg_df['subj_original_text'] = kg_df['subj_original_text'].str.lower()\n",
    "kg_df['obj_original_text'] = kg_df['obj_original_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning data:\n",
      "number of nodes = 50182\n",
      "number of unique relationship types = 31\n",
      "number of unique edges (includes all edge properties) = 918936\n",
      "number of unique triples = 373327\n",
      "number of unique paper IDs = 208107\n"
     ]
    }
   ],
   "source": [
    "print('After cleaning data:')\n",
    "\n",
    "nodes = set(kg_df[subj_col]).union(set(kg_df[obj_col]))\n",
    "print(\"number of nodes =\", len(nodes))\n",
    "rels = set(kg_df[rel_col])\n",
    "rels_standardized = set([r.replace('(SPEC)', \"\").replace('(INFER)', \"\") for r in rels])\n",
    "print(\"number of unique relationship types =\", len(rels_standardized))\n",
    "print(\"number of unique edges (includes all edge properties) =\", len(kg_df))\n",
    "num_relationships = len(kg_df.groupby([subj_col, rel_col, obj_col]).count())\n",
    "print(\"number of unique triples =\", num_relationships)\n",
    "\n",
    "print('number of unique paper IDs =', len(set(kg_df['paper_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gene name as preferred name\n",
    "def set_gene_name_as_preferred_name(row, pref_name_col, gene_name_col):\n",
    "    if str(row[gene_name_col]) != 'nan':\n",
    "        return row[gene_name_col]\n",
    "    else:\n",
    "        return row[pref_name_col]\n",
    "    \n",
    "kg_df[subj_col] = kg_df.apply(lambda row: set_gene_name_as_preferred_name(row, subj_col, 'subj_gene_name'), axis=1)\n",
    "kg_df[obj_col] = kg_df.apply(lambda row: set_gene_name_as_preferred_name(row, obj_col, 'obj_gene_name'), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223 SARS-CoV-2 (virus) terms\n"
     ]
    }
   ],
   "source": [
    "# SARS-CoV-2 terms\n",
    "coronavirus = []\n",
    "with open(os.path.join(\"covid_19_dictionaries\", \"Virus_SARS-CoV-2.txt\")) as f:\n",
    "    for line in f:\n",
    "        coronavirus.append(line.strip())\n",
    "print(len(coronavirus), \"SARS-CoV-2 (virus) terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192 COVID-19 (disease) terms (partial match)\n",
      "11723 COVID-19 (disease) terms (long/exact match)\n",
      "12915 COVID-19 (disease) terms (all)\n"
     ]
    }
   ],
   "source": [
    "# COVID-19 terms\n",
    "covid = []\n",
    "covid_exact = []\n",
    "covid_all = []\n",
    "with open(os.path.join(\"covid_19_dictionaries\", \"Disease_COVID-19.txt\")) as f:\n",
    "    for line in f:\n",
    "        term = line.strip()\n",
    "        covid_all.append(term)\n",
    "        if term in [\"wurs\", \"ncp\", \"sars2\"] or term.count(\" \") >= 4:\n",
    "            covid_exact.append(term)\n",
    "        else:\n",
    "            covid.append(term)\n",
    "print(len(covid), \"COVID-19 (disease) terms (partial match)\")\n",
    "print(len(covid_exact), \"COVID-19 (disease) terms (long/exact match)\")\n",
    "print(len(covid_all), \"COVID-19 (disease) terms (all)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for standardization of COVID-19-related entities\n",
    "temp = pd.read_csv('table_s1.csv')\n",
    "d_replace = dict(zip(list(temp['original term']), list(temp['normalized'])))\n",
    "virus_terms = []\n",
    "covid_terms = []\n",
    "        \n",
    "for k in d_replace:\n",
    "    if 'SARS-CoV-2' in d_replace[k]:\n",
    "        virus_terms.append(k)\n",
    "    elif 'COVID-19' in d_replace[k]:\n",
    "        covid_terms.append(k)\n",
    "    else:\n",
    "        raise Exception('Normalized term must contain SARS-CoV-2 or COVID-19', d[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update semantic type for COVID-19-related terms\n",
    "# either 'SARS-CoV-2 (virus)' or 'COVID-19 (disease)'\n",
    "\n",
    "def update_covid_semtype(row, original_text_col, semtype_col):\n",
    "    if str(row[original_text_col]).lower() in virus_terms:\n",
    "        return \"SARS-CoV-2 (virus)\"\n",
    "    elif str(row[original_text_col]).lower() in covid_terms:\n",
    "        return \"COVID-19 (disease)\"\n",
    "    else:\n",
    "        return row[semtype_col]\n",
    "\n",
    "kg_df['subj_semantic_type'] = kg_df.apply(lambda x: update_covid_semtype(x, 'subj_original_text', 'subj_semantic_type'), axis=1)\n",
    "kg_df['obj_semantic_type'] = kg_df.apply(lambda x: update_covid_semtype(x, 'obj_original_text', 'obj_semantic_type'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add COVID-19-related preferred names based on standardization\n",
    "def update_covid_preferred_name(row, original_text_col, semantic_type_col, preferred_name_col):\n",
    "    if str(row[semantic_type_col]) == \"SARS-CoV-2 (virus)\" or str(row[semantic_type_col]) == \"COVID-19 (disease)\":\n",
    "        return str(row[original_text_col]).lower()\n",
    "    else:\n",
    "        return row[preferred_name_col]\n",
    "    \n",
    "kg_df[subj_col] = kg_df.apply(lambda x: update_covid_preferred_name(x, \"subj_original_text\", \"subj_semantic_type\", subj_col), axis=1)\n",
    "kg_df[obj_col] = kg_df.apply(lambda x: update_covid_preferred_name(x, \"obj_original_text\", \"obj_semantic_type\", obj_col), axis=1)\n",
    "\n",
    "kg_df[subj_col] = kg_df[subj_col].replace(d_replace) \n",
    "kg_df[obj_col] = kg_df[obj_col].replace(d_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23 coronavirus-related terms (preferred name after normalizing COVID-19 and SARS-CoV-2)\n",
      "{'COVID-19 subject', 'COVID-19 symptom', 'COVID-19 country', 'COVID-19 co-infection', 'SARS-CoV-2 PCR', 'COVID-19 re-infection', 'COVID-19 vaccine', 'SARS-CoV-2 spread', 'COVID-19 animal', 'SARS-CoV-2', 'COVID-19', 'COVID-19 treatment', 'COVID-19 related infection', 'SARS-CoV-2 protein', 'COVID-19 testing', 'SARS-CoV-2 gene', 'Asymptomatic COVID-19', 'COVID-19 acquired infection', 'COVID-19 suspected', 'COVID-19 patient', 'Severe COVID-19', 'SARS-CoV-2 antibody', 'SARS-CoV-2 related virus'}\n"
     ]
    }
   ],
   "source": [
    "covid_semtypes = [\"SARS-CoV-2 (virus)\", \"COVID-19 (disease)\"] # custom semantic types representing coronavirus/covid\n",
    "coronavirus_subj_terms = set(kg_df[kg_df[\"subj_semantic_type\"].isin(covid_semtypes)][subj_col])\n",
    "coronavirus_obj_terms = set(kg_df[kg_df[\"obj_semantic_type\"].isin(covid_semtypes)][obj_col])\n",
    "coronavirus_terms = coronavirus_subj_terms.union(coronavirus_obj_terms)\n",
    "\n",
    "print(\"There are\", len(coronavirus_terms), \"coronavirus-related terms (preferred name after normalizing COVID-19 and SARS-CoV-2)\")\n",
    "print(coronavirus_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map semtype abbreviations to semantic group\n",
    "semtypes_df = pd.read_csv(os.path.join('semantic_type_files', 'SemanticTypes_2018AB.txt'), sep=\"|\", header=None)\n",
    "semgroups_df = pd.read_csv(os.path.join('semantic_type_files', 'SemGroups_2018.txt'), sep=\"|\", header=None)\n",
    "semgroups_df.columns = ['semgroup_abbreviation', 'semgroup', 'TUI', 'semtype']\n",
    "semtypes_df.columns = ['semtype_abbreviation', 'TUI', 'semtype']\n",
    "semtypes_df = semtypes_df.merge(semgroups_df, on=['TUI', 'semtype'])\n",
    "\n",
    "keys = semtypes_df[\"semtype_abbreviation\"]\n",
    "values = semtypes_df[\"semgroup\"]\n",
    "semtype_dictionary = dict(zip(keys, values))\n",
    "\n",
    "kg_df.replace({'subj_semantic_type': semtype_dictionary}, inplace=True)\n",
    "kg_df.replace({'obj_semantic_type': semtype_dictionary}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-825388796121>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  kg_df[rel_col] = kg_df[rel_col].str.replace(\"\\(SPEC\\)\", \"\")\n",
      "<ipython-input-18-825388796121>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  kg_df[rel_col] = kg_df[rel_col].str.replace(\"\\(INFER\\)\", \"\")\n"
     ]
    }
   ],
   "source": [
    "kg_df[rel_col] = kg_df[rel_col].str.replace(\"\\(SPEC\\)\", \"\")\n",
    "kg_df[rel_col] = kg_df[rel_col].str.replace(\"\\(INFER\\)\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of relationships (after removing low confidence entities) = 837078\n"
     ]
    }
   ],
   "source": [
    "# get most confidence and non-negated relationships\n",
    "kg_df = kg_df[(kg_df[\"subj_confidence_score\"]>=800)&(kg_df[\"obj_confidence_score\"]>=800)]\n",
    "print(\"number of relationships (after removing low confidence entities) =\", len(kg_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of relationships (after removing self-loops) = 835010\n"
     ]
    }
   ],
   "source": [
    "# remove self-loops\n",
    "kg_df = kg_df[(kg_df[subj_col]!=kg_df[obj_col])&(kg_df['subj_CUI']!=kg_df['obj_CUI'])]\n",
    "print(\"number of relationships (after removing self-loops) =\", len(kg_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before thresholding ===\n",
      "Number of edges (sum of weights): 835010\n",
      "Number of unique edges: 340193\n",
      "\n",
      "=== After thresholding ===\n",
      "Number of edges (sum of weights): 590925\n",
      "Number of unique edges: 96108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter relationships to only keep triples that occur at least twice\n",
    "weight_col = 'weight'\n",
    "weight_df = kg_df.groupby([subj_col, rel_col, obj_col])['subj_start'].count().reset_index().rename(columns={'subj_start': weight_col})\n",
    "thresh = 2\n",
    "weight_thresh_df = weight_df[weight_df[weight_col] >= thresh]\n",
    "kg_df = kg_df.merge(weight_thresh_df, on=[subj_col, rel_col, obj_col], how='inner')\n",
    "\n",
    "print('=== Before thresholding ===')\n",
    "print('Number of edges (sum of weights):', sum(weight_df[weight_col]))\n",
    "print('Number of unique edges:', len(weight_df))\n",
    "print()\n",
    "print('=== After thresholding ===')\n",
    "print('Number of edges (sum of weights):', sum(weight_thresh_df[weight_col]))\n",
    "print('Number of unique edges:', len(weight_thresh_df))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of relationships (after dropping duplicates) = 590923\n"
     ]
    }
   ],
   "source": [
    "# drop_duplicates()\n",
    "kg_df = kg_df.drop_duplicates(subset=['subj_start', 'subj_end', 'obj_start', 'obj_end', 'relation_start', 'relation_end', 'sentence_lower', subj_col, rel_col, obj_col])\n",
    "print(\"number of relationships (after dropping duplicates) =\", len(kg_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Text entities/nodes and Text-Document relationships for Neo4j Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text/sentence information from triple\n",
    "sentences_id_df = kg_df[['paper_id', 'sentence']].drop_duplicates().reset_index(drop=True).reset_index().rename(columns={'index': 'text_id'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_id_df['text_id'] = sentences_id_df['text_id'].astype(int)\n",
    "sentences_df = sentences_id_df.copy()\n",
    "sentences_df = sentences_df[['text_id', 'sentence']]\n",
    "sentences_df.columns = ['text_id:ID(Text)', 'text:STRING']\n",
    "sentences_df['section:STRING'] = 'abstract'\n",
    "sentences_df[':LABEL'] = 'Text'\n",
    "sentences_df.to_csv(os.path.join(NEO4J_OUTPUT_DIR, 'text_nodes.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-Document relationships\n",
    "kg_df_sent = kg_df.merge(sentences_id_df, on=['sentence', 'paper_id'], how='left')\n",
    "sentences_edges_df = kg_df_sent[['text_id', 'paper_id']].drop_duplicates()    \n",
    "sentences_edges_df.columns = ['text_id:START_ID(Text)', 'doc_id:END_ID(Document)']\n",
    "sentences_edges_df[':TYPE'] = 'in_document'\n",
    "sentences_edges_df.to_csv(os.path.join(NEO4J_OUTPUT_DIR, 'text_edges.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create file for processed triples and their corresponding entities/nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns in the edges file\n",
    "id_col = 'id' # the preferred name of the entity\n",
    "id_col_1 = id_col + \"1\"\n",
    "id_col_2 = id_col + \"2\"\n",
    "\n",
    "identifier_col = 'identifier' # the CUI of the entity\n",
    "identifier_col_1 = identifier_col + \"1\"\n",
    "identifier_col_2 = identifier_col + \"2\"\n",
    "\n",
    "type_col = 'type' # the semantic group of the entity\n",
    "type_col_1 = type_col + \"1\"\n",
    "type_col_2 = type_col + \"2\"\n",
    "\n",
    "info_col = 'info' # synonyms/original texts for the entity\n",
    "info_col_1 = info_col + \"1\"\n",
    "info_col_2 = info_col + \"2\"\n",
    "\n",
    "source_col = 'evidence_source' # what source the triple is from\n",
    "rel_col = ':TYPE' # relationship\n",
    "\n",
    "# what to name the columns after aggregation\n",
    "identifier_col_agg = identifier_col + \"s\"\n",
    "type_col_agg = type_col + \"s\"\n",
    "info_col_agg = info_col + \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update headers\n",
    "kg_df = kg_df_sent\n",
    "header = ['subj_preferred_name', 'subj_CUI', 'subj_semantic_type', 'subj_original_text', 'subj_start', 'subj_end',\n",
    "          'obj_preferred_name', 'obj_CUI', 'obj_semantic_type', 'obj_original_text', 'obj_start', 'obj_end',\n",
    "          'relation', 'relation_start', 'relation_end', 'text_id', weight_col]\n",
    "header_updated = [id_col_1, identifier_col_1, type_col_1, info_col_1, 'start1', 'end1',\n",
    "                  id_col_2, identifier_col_2, type_col_2, info_col_2, 'start2', 'end2',\n",
    "                  rel_col, 'startr', 'endr', 'text_id', weight_col]\n",
    "kg_df = kg_df.loc[:,header]\n",
    "kg_df.columns = header_updated\n",
    "kg_df[source_col] = 'CORD-19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect nodes from triples\n",
    "temp1 = kg_df[[identifier_col_1, id_col_1, type_col_1, info_col_1]].rename(columns={identifier_col_1: identifier_col, id_col_1: id_col, info_col_1: info_col, type_col_1: type_col})\n",
    "temp2 = kg_df[[identifier_col_2, id_col_2, type_col_2, info_col_2]].rename(columns={identifier_col_2: identifier_col, id_col_2: id_col, info_col_2: info_col, type_col_2: type_col})\n",
    "node_df = temp1.append(temp2).drop_duplicates().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group entities by id (preferred name) and collect different identifiers, types, and infos\n",
    "temp = node_df.groupby(id_col, as_index=False).agg({identifier_col: lambda x: ';'.join(set(x)), type_col: lambda x: ';'.join(set(x)), info_col: lambda x: ';'.join(set(x))})\n",
    "node_id_mapping_df = node_df[[identifier_col, id_col, type_col, info_col]].merge(temp, on=[id_col])\n",
    "node_id_mapping_df.columns = [identifier_col, id_col, type_col, info_col, identifier_col_agg, type_col_agg, info_col_agg]\n",
    "node_id_mapping_df.to_csv(NODE_MAPPING_FILE_PATH , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique nodes: 21694\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique nodes:', len(set(node_id_mapping_df[id_col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the node information in the triples\n",
    "kg_df = kg_df.merge(node_id_mapping_df[[id_col, info_col_agg, type_col_agg, identifier_col_agg]].drop_duplicates(), left_on=id_col_1, right_on=id_col, how='left').drop([id_col, identifier_col_1, type_col_1, info_col_1], axis=1).rename(columns={identifier_col_agg: identifier_col_1, type_col_agg: type_col_1, info_col_agg: info_col_1})\n",
    "kg_df = kg_df.merge(node_id_mapping_df[[id_col, info_col_agg, type_col_agg, identifier_col_agg]].drop_duplicates(), left_on=id_col_2, right_on=id_col, how='left').drop([id_col, identifier_col_2, type_col_2, info_col_2], axis=1).rename(columns={identifier_col_agg: identifier_col_2, type_col_agg: type_col_2, info_col_agg: info_col_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved the processed triples\n",
    "kg_df.to_csv(OUTPUT_FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>start1</th>\n",
       "      <th>end1</th>\n",
       "      <th>id2</th>\n",
       "      <th>start2</th>\n",
       "      <th>end2</th>\n",
       "      <th>:TYPE</th>\n",
       "      <th>startr</th>\n",
       "      <th>endr</th>\n",
       "      <th>text_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>evidence_source</th>\n",
       "      <th>info1</th>\n",
       "      <th>type1</th>\n",
       "      <th>identifier1</th>\n",
       "      <th>info2</th>\n",
       "      <th>type2</th>\n",
       "      <th>identifier2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diagnosis</td>\n",
       "      <td>465</td>\n",
       "      <td>474</td>\n",
       "      <td>Patients</td>\n",
       "      <td>552</td>\n",
       "      <td>560</td>\n",
       "      <td>ADMINISTERED_TO</td>\n",
       "      <td>534</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>CORD-19</td>\n",
       "      <td>diagnosing;pnd;diagnoses;diagnostics;diagnosti...</td>\n",
       "      <td>Procedures</td>\n",
       "      <td>C0011900</td>\n",
       "      <td>ipd;patients;cpps;aisp;esp;ip;tp;sp;ncip;asp;p...</td>\n",
       "      <td>Living Beings</td>\n",
       "      <td>C0030705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diagnosis</td>\n",
       "      <td>103</td>\n",
       "      <td>112</td>\n",
       "      <td>Patients</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>ADMINISTERED_TO</td>\n",
       "      <td>82</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>CORD-19</td>\n",
       "      <td>diagnosing;pnd;diagnoses;diagnostics;diagnosti...</td>\n",
       "      <td>Procedures</td>\n",
       "      <td>C0011900</td>\n",
       "      <td>ipd;patients;cpps;aisp;esp;ip;tp;sp;ncip;asp;p...</td>\n",
       "      <td>Living Beings</td>\n",
       "      <td>C0030705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Diagnosis</td>\n",
       "      <td>99</td>\n",
       "      <td>108</td>\n",
       "      <td>Patients</td>\n",
       "      <td>186</td>\n",
       "      <td>194</td>\n",
       "      <td>ADMINISTERED_TO</td>\n",
       "      <td>168</td>\n",
       "      <td>178</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>CORD-19</td>\n",
       "      <td>diagnosing;pnd;diagnoses;diagnostics;diagnosti...</td>\n",
       "      <td>Procedures</td>\n",
       "      <td>C0011900</td>\n",
       "      <td>ipd;patients;cpps;aisp;esp;ip;tp;sp;ncip;asp;p...</td>\n",
       "      <td>Living Beings</td>\n",
       "      <td>C0030705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diagnosis</td>\n",
       "      <td>213</td>\n",
       "      <td>222</td>\n",
       "      <td>Patients</td>\n",
       "      <td>185</td>\n",
       "      <td>193</td>\n",
       "      <td>ADMINISTERED_TO</td>\n",
       "      <td>194</td>\n",
       "      <td>203</td>\n",
       "      <td>3</td>\n",
       "      <td>136</td>\n",
       "      <td>CORD-19</td>\n",
       "      <td>diagnosing;pnd;diagnoses;diagnostics;diagnosti...</td>\n",
       "      <td>Procedures</td>\n",
       "      <td>C0011900</td>\n",
       "      <td>ipd;patients;cpps;aisp;esp;ip;tp;sp;ncip;asp;p...</td>\n",
       "      <td>Living Beings</td>\n",
       "      <td>C0030705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diagnosis</td>\n",
       "      <td>51</td>\n",
       "      <td>60</td>\n",
       "      <td>Patients</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>ADMINISTERED_TO</td>\n",
       "      <td>40</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>CORD-19</td>\n",
       "      <td>diagnosing;pnd;diagnoses;diagnostics;diagnosti...</td>\n",
       "      <td>Procedures</td>\n",
       "      <td>C0011900</td>\n",
       "      <td>ipd;patients;cpps;aisp;esp;ip;tp;sp;ncip;asp;p...</td>\n",
       "      <td>Living Beings</td>\n",
       "      <td>C0030705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id1  start1  end1       id2  start2  end2            :TYPE  startr  \\\n",
       "0  Diagnosis     465   474  Patients     552   560  ADMINISTERED_TO     534   \n",
       "1  Diagnosis     103   112  Patients      24    32  ADMINISTERED_TO      82   \n",
       "2  Diagnosis      99   108  Patients     186   194  ADMINISTERED_TO     168   \n",
       "3  Diagnosis     213   222  Patients     185   193  ADMINISTERED_TO     194   \n",
       "4  Diagnosis      51    60  Patients       0     8  ADMINISTERED_TO      40   \n",
       "\n",
       "   endr  text_id  weight evidence_source  \\\n",
       "0   544        0     136         CORD-19   \n",
       "1    91        1     136         CORD-19   \n",
       "2   178        2     136         CORD-19   \n",
       "3   203        3     136         CORD-19   \n",
       "4    47        4     136         CORD-19   \n",
       "\n",
       "                                               info1       type1 identifier1  \\\n",
       "0  diagnosing;pnd;diagnoses;diagnostics;diagnosti...  Procedures    C0011900   \n",
       "1  diagnosing;pnd;diagnoses;diagnostics;diagnosti...  Procedures    C0011900   \n",
       "2  diagnosing;pnd;diagnoses;diagnostics;diagnosti...  Procedures    C0011900   \n",
       "3  diagnosing;pnd;diagnoses;diagnostics;diagnosti...  Procedures    C0011900   \n",
       "4  diagnosing;pnd;diagnoses;diagnostics;diagnosti...  Procedures    C0011900   \n",
       "\n",
       "                                               info2          type2  \\\n",
       "0  ipd;patients;cpps;aisp;esp;ip;tp;sp;ncip;asp;p...  Living Beings   \n",
       "1  ipd;patients;cpps;aisp;esp;ip;tp;sp;ncip;asp;p...  Living Beings   \n",
       "2  ipd;patients;cpps;aisp;esp;ip;tp;sp;ncip;asp;p...  Living Beings   \n",
       "3  ipd;patients;cpps;aisp;esp;ip;tp;sp;ncip;asp;p...  Living Beings   \n",
       "4  ipd;patients;cpps;aisp;esp;ip;tp;sp;ncip;asp;p...  Living Beings   \n",
       "\n",
       "  identifier2  \n",
       "0    C0030705  \n",
       "1    C0030705  \n",
       "2    C0030705  \n",
       "3    C0030705  \n",
       "4    C0030705  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
